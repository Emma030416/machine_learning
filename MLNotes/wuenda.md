# Content
- [Introduction](#introduction)
  - [ğŸ“Œ supervised learning](#-supervised-learning)
  - [ğŸ“Œ unsupervised learning](#-unsupervised-learning)
- [Linear Regression with One Variable](#linear-regression-with-one-variable)
  - [ğŸ“Œ model](#-model)
  - [ğŸ“Œ cost function for Linear Regression with One Variable](#-cost-function-for-linear-regression-with-one-variable)
  - [ğŸ“Œ gradient decent](#-gradient-decent)
- [Linear Algebra](#linear-algebra)
- [Linear Regression with Multiple Variables](#linear-regression-with-multiple-variables)
  - [ğŸ“Œ model](#-model-1)
  - [ğŸ“Œ cost function and gradient decent](#-cost-function-and-gradient-decent)
  - [ğŸ“Œ feature scaling](#-feature-scaling)
  - [ğŸ“Œ polynomial regression](#-polynomial-regression)
  - [ğŸ“Œ normal equation](#-normal-equation)
- [Logistic Regression](#logistic-regression)
  - [ğŸ“Œ hypothesis function](#-hypothesis-function)
  - [ğŸ“Œ decision boundary](#-decision-boundary)
  - [ğŸ“Œ cost function for Logistic Regression](#-cost-function-for-logistic-regression)
  - [ğŸ“Œ advanced optimization](#-advanced-optimization)
  - [ğŸ“Œ multiclass classification](#-multiclass-classification)
- [Regularization](#regularization)
  - [ğŸ“Œ the problem of overfitting](#-the-problem-of-overfitting)
  - [ğŸ“Œ cost function for Regularization](#-cost-function-for-regularization)
  - [ğŸ“Œ regularized Linear Regression](#-regularized-linear-regression)
  - [ğŸ“Œ regularized Logistic Regression](#-regularized-logistic-regression)

# Introduction
machine learning algorithms:<br>
supervised learning <-> unsupervised learning
![æè¿°](./img/wuenda01.png)

## ğŸ“Œ supervised learning
input-> output label<br>
learn from being given "right answers"

applications using supervised learning:

| Input (X)         | Output (Y)             | Application         |
| ----------------- | ---------------------- | ------------------- |
| email             | spam? (0/1)            | **spam filtering**      |
| audio             | text transcripts       | speech recognition  |
| English           | Spanish                | machine translation |
| ad, user info     | click? (0/1)           | online advertising  |
| image, radar info | position of other cars | self-driving car    |
| image of phone    | defect? (0/1)          | visual inspection   |

examples:<br>
1. housing price prediction:<br>
whether to fit a straight line, a curve or another function to the data
![æè¿°](./img/wuenda02.png)
âœ… **regression**: predict numbers / continuous valued output

2. breast cancer detection:<br>
![æè¿°](./img/wuenda03.png)
âœ… **classification**: predict categories / discrete valued output

the examples above only provide one input or feature, in fact, more than one feature also works<br>
![æè¿°](./img/wuenda04.png)
find a boundary line<br>
âœ… we use **SVM(Support Vector Machine)** when we have infinite numbers of features

## ğŸ“Œ unsupervised learning
find the structure or pattern by itself in unlabeled data

âœ… **clustering**: group data into different clusters

examples:<br>
1. google news:<br>
find the acticles with similar words and group them into the same cluster
this is used in Recommender Systems, recommend related articles in the same cluster

2. Cocktail Party Algorithm:<br>
for knowing only<br>
a typical problem of BSS(Blind Source Separation), solving by ICA or Sparse Coding)

# Linear Regression with One Variable
## ğŸ“Œ model
example: housing price prediction

m: number of training examples<br>
x: input(feature)<br>
y: output(target/label)<br>
(x, y): one training example<br>
(xâ½â±â¾, yâ½â±â¾): the ith training example<br>
h: function of the model(f(x))

so the model is like below:
![æè¿°](./img/wuenda05.png)

## ğŸ“Œ cost function for Linear Regression with One Variable
after setting up our model, we need to choose the reasonable parameters: Î¸o and Î¸1

what means reasonable? --- minimize the modeling error between predicted output and real output

we use **cost funtion**(ä»£ä»·å‡½æ•°) to measure the error
![æè¿°](./img/wuenda06.png)

âœ… the cost function, also called "the square error function"(å‡æ–¹è¯¯å·®å‡½æ•°), uses the **least squares method**(æœ€å°äºŒä¹˜æ³•)

m is for averaging, making the function independent of the sample size;<br>
2 is to ensure that the gradient grad after differentiation has no extra coefficients, which cancel out of the square 2

to better visualize the function, let's simplify it first:
![æè¿°](./img/wuenda07.png)
![æè¿°](./img/wuenda08.png)

if we have two parameters, the picture would look like this :
![æè¿°](./img/wuenda09.png)
in three-dimensional space, we can still find the lowest point
![æè¿°](./img/wuenda10.png)

to automatically find the parameters that minimize the cost function J, we introduce gradient decent

## ğŸ“Œ gradient decent
âœ… **gradient decent**(æ¢¯åº¦ä¸‹é™) is used to minimize some arbitrary funciton

imagine you are on a hill, to go down the hill as quickly as possible, you need to look around and find the best direction then take a step.<br>
Then you keep going, from this new point you are now standing at, look around and find the best direction then take another step...

starting with different points of the hill, you'll end up with different local minimum / `local optimum`(å±€éƒ¨æœ€å°å€¼/å±€éƒ¨æœ€ä¼˜è§£)

here is the visualized picture:
![æè¿°](./img/wuenda11.png)

here is the algorithm:
![æè¿°](./img/wuenda12.png)

something you need to know in the algorithm:<br>
1. derivative term(å¯¼æ•°é¡¹)
![æè¿°](./img/wuenda13.png)
![æè¿°](./img/wuenda16.png)
the derivative term will be smaller and smaller

2. Î± is learning rate, it controls how big a step we take<br>
![æè¿°](./img/wuenda14.png)
you can try 0.001, 0.01, 0.1, 1...

3. if you don't update simultaneous(åŒæ­¥æ›´æ–°), say you update Î¸o first, then when you update Î¸1, now Î¸o in J(Î¸o, Î¸1) will be the updated Î¸o, and this is incorrect
4. what if you are already on the minimum point at first?
![æè¿°](./img/wuenda15.png)

now let's see how to use grdient decent to minimize the cost function J:
![æè¿°](./img/wuenda17.png)
![æè¿°](./img/wuenda18.png)

don't worry about getting the local optimum, cause the cost function is always a bow shape<br>
there is only a single optimum, that is the global optimum(å…¨å±€æœ€ä¼˜è§£)
![æè¿°](./img/wuenda09.png)

as for this picture we see before, the right one is a Contour map(ç­‰å€¼çº¿å›¾), every point on the same circular line has the same cost<br>
as the point goes closer and closer to the center point(minimum cost), the cost becomes smaller, and the line on the left picture better fit the data
![æè¿°](./img/wuenda10.png)

# Linear Algebra
how to use Linear Algebra to simplify our Linear Regression model's calculation?
![æè¿°](./img/wuenda19.png)
![æè¿°](./img/wuenda20.png)

review of linear algebra:
![æè¿°](./img/wuenda21.png)
![æè¿°](./img/wuenda22.png)
![æè¿°](./img/wuenda23.png)

# Linear Regression with Multiple Variables
## ğŸ“Œ model
![æè¿°](./img/wuenda24.png)
![æè¿°](./img/wuenda25.png)

## ğŸ“Œ cost function and gradient decent
![æè¿°](./img/wuenda26.png)
![æè¿°](./img/wuenda27.png)

## ğŸ“Œ feature scaling
if two parameters are not on the same scale(é‡çº§), it will take a long time to find its way to the global minimum
![æè¿°](./img/wuenda30.png)

here are two solutions:
1. âœ… **normalization**
![æè¿°](./img/wuenda28.png)
in fact, just in a particular small range, like [-1,1], [-0.5,0.5], these are all OK<br>
but like [-100,100], [-0.0001,0.0001], you need to consider it then

a simple way to do this is just dividing by its maximum value:
![æè¿°](./img/wuenda31.png)

2. âœ… **standardization**
![æè¿°](./img/wuenda29.png)

## ğŸ“Œ polynomial regression
as for polynomial regression(å¤šé¡¹å¼å›å½’), we can turn it to linear regression as below:
![æè¿°](./img/wuenda32.png)

## ğŸ“Œ normal equation
âœ… **normal equation**(æ­£è§„æ–¹ç¨‹) is another way to find the potimal parameters, in some case better than gradient decent
![æè¿°](./img/wuenda33.png)
all you need to do is set the differentiation(å¯¼æ•°) as 0(same as finding the maximum/minimum in maths)

![æè¿°](./img/wuenda34.png)
![æè¿°](./img/wuenda35.png)

if the matrix is non-invertible(ä¸å¯é€†), normal equation can't be used

normal equation is only applicable to linear models, not for other models such as logistic regression models

# Logistic Regression
logistic regression is **classification** problem
let's start with the binary classification problem(äºŒåˆ†ç±»é—®é¢˜)

## ğŸ“Œ hypothesis function
we use the **sigmoid function**<br>
positive numbers -> 1 and negative numbers -> 0
![æè¿°](./img/wuenda36.png)

we can adjust the parameters according to the actual problem<br>
by adding w, horizontal stretching or compression(æ¨ªå‘æ‹‰ä¼¸/å‹ç¼©) can be achieved
![æè¿°](./img/wuenda37.png)

then we can add the bias b
![æè¿°](./img/wuenda38.png)
it can be seen that logistic regression is to input the result of linear regression into the sigmoid function, and map it between 0 and 1

âœ… if we represent it with a variable matrix and a parameter matrix, **general hypothesis function of logistic regression**(é€»è¾‘å›å½’çš„é€šç”¨å‡è®¾å‡½æ•°) is achieved
![æè¿°](./img/wuenda39.png)

how to understand the output of the hypothesis function?
![æè¿°](./img/wuenda40.png)

in python, we can achieve this function in this way:

```python
import numpy as np 
def sigmoid(z): 
 return 1 / (1 + np.exp(-z))
```

## ğŸ“Œ decision boundary
![æè¿°](./img/wuenda41.png)
the decision boundary(å†³ç­–è¾¹ç•Œ) can be a straight line
![æè¿°](./img/wuenda42.png)
it can also be a curve(when there's higher-order term)
![æè¿°](./img/wuenda43.png)

## ğŸ“Œ cost function for Logistic Regression
![æè¿°](./img/wuenda44.png)

for linear regression models, the cost function we define is the square error function<br>
theoretically speaking, we can follow this definition, but the hypothesis function of logistic regression is very complex, so the cost function we obtained wiil be a **non-convex function**(éå‡¸å‡½æ•°)<br>
this means that our cost function has many local minimum, which will affect our using gradient descent algorithm to search for the global minimum
![æè¿°](./img/wuenda45.png)

so, we change the cost function
![æè¿°](./img/wuenda46.png)
![æè¿°](./img/wuenda47.png)

this cost function can be derived from the principle of **maximum likelihood estimation**(æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ³•)

so let's use gradient decent to see the minimum of cost function
![æè¿°](./img/wuenda48.png)
pay attention that the h(x) here is the sigmoid function<br>
after calculating, we find that `the result is exactly the same as linear regression`!

## ğŸ“Œ advanced optimization
just for knowing
![æè¿°](./img/wuenda51.png)

## ğŸ“Œ multiclass classification
also called one-vs-all<br>
âœ… turn multiclass into two classes!
![æè¿°](./img/wuenda49.png)

to visualize it, here's an example:
![æè¿°](./img/wuenda50.png)

# Regularization
regularization(æ­£åˆ™åŒ–) can reduce the problem of overfitting
## ğŸ“Œ the problem of overfitting
![æè¿°](./img/wuenda52.png)
![æè¿°](./img/wuenda53.png)
it can be seen that if the power(æ¬¡å¹‚) of x is too high, it may lead to overfitting

how to solve this problem?
![æè¿°](./img/wuenda54.png)

## ğŸ“Œ cost function for Regularization
From the previous examples, we can see that if the power(æ¬¡å¹‚) of x is too high, it may lead to overfitting

so if the coefficients of these higher-order terms(é«˜é¡¹å¼) approach 0, we can fit them very well

we can add prenalize(æƒ©ç½š)
![æè¿°](./img/wuenda55.png)

the value of regularization parameter is important
![æè¿°](./img/wuenda56.png)

## ğŸ“Œ regularized Linear Regression
gradient decent:
![æè¿°](./img/wuenda57.png)

normal equation:
![æè¿°](./img/wuenda58.png)

## ğŸ“Œ regularized Logistic Regression
gradient decent:
![æè¿°](./img/wuenda59.png)
![æè¿°](./img/wuenda60.png)

advanced optimization:
![æè¿°](./img/wuenda61.png)
