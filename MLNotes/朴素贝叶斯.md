## åŸºç¡€çŸ¥è¯†
### è´å¶æ–¯å…¬å¼

![æè¿°](./img/æœ´ç´ è´å¶æ–¯01.png)

å…ˆéªŒæ¦‚ç‡ï¼šäº‹ä»¶å‘ç”Ÿçš„åˆå§‹æ¦‚ç‡(C)

åéªŒæ¦‚ç‡ï¼šåœ¨å·²çŸ¥æŸäº›æ¡ä»¶æˆ–è¯æ®(W)åï¼Œäº‹ä»¶å‘ç”Ÿçš„ä¿®æ­£æ¦‚ç‡

### æœ´ç´ è´å¶æ–¯
åœ¨è´å¶æ–¯çš„åŸºç¡€ä¸Šï¼Œå¢åŠ ä¸€ä¸ªæ¡ä»¶ï¼š**ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹**

å®é™…è®¡ç®—ä¸­ï¼Œä¸ºäº†é¿å…å‡ºç°é›¶æ¦‚ç‡æƒ…å†µï¼Œä¼šå¢åŠ ï¼ˆæ‹‰æ™®æ‹‰æ–¯ï¼‰å¹³æ»‘ç³»æ•°

å…¬å¼ï¼š
![æè¿°](./img/æœ´ç´ è´å¶æ–¯02.png)

#### ğŸ“Œ å‚æ•°è§£é‡Šï¼š

P(F1|C)ï¼šåœ¨ç±»åˆ«Cçš„æ¡ä»¶ä¸‹ï¼Œç‰¹å¾F1å‡ºç°çš„æ¡ä»¶æ¦‚ç‡

Niï¼šç±»åˆ«Cä¸­ï¼Œç‰¹å¾F1å®é™…å‡ºç°çš„æ¬¡æ•°

Nï¼šç±»åˆ«Cä¸­æ‰€æœ‰ç‰¹å¾å‡ºç°çš„æ€»æ¬¡æ•°

mï¼šç‹¬ç«‹ç‰¹å¾çš„æ€»æ•°ï¼ˆç‰¹å¾ç©ºé—´çš„å¤§å°ï¼‰

Î±ï¼šå¹³æ»‘ç³»æ•°ï¼Œé€šå¸¸å–1

#### ğŸ“Œ ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾ä½ åœ¨åšä¸€ä¸ªåƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ï¼Œç±»åˆ« C æ˜¯â€œåƒåœ¾é‚®ä»¶â€

æ€»å…±æœ‰ 1000 ä¸ªè¯å‡ºç°åœ¨åƒåœ¾é‚®ä»¶ä¸­ï¼ˆN=1000ï¼‰

å…¶ä¸­â€œå…è´¹â€è¿™ä¸ªè¯å‡ºç°äº† 20 æ¬¡ï¼ˆNi=20ï¼‰

è¯æ±‡è¡¨é‡Œå…±æœ‰ 5000 ä¸ªä¸åŒçš„è¯ï¼ˆm=5000ï¼‰

å¹³æ»‘ç³»æ•° Î±=1

é‚£ä¹ˆï¼š

![æè¿°](./img/æœ´ç´ è´å¶æ–¯03.png)

## å®æˆ˜æ¡ˆä¾‹
æƒ…æ„Ÿåˆ†ç±»æ¡ˆä¾‹

æ ¹æ®å•†å“è¯„è®ºå†…å®¹åˆ¤æ–­æƒ…æ„Ÿå€¾å‘ï¼ˆå¥½è¯„/å·®è¯„ï¼‰

å±äºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­å…¸å‹çš„**åˆ†ç±»é—®é¢˜**

### æµç¨‹åˆ†æ
#### ğŸ“Œ è·å–æ•°æ®

``` python
# pd.read_csvè¯»å–csvæ–‡ä»¶æ•°æ®ï¼Œgbkç”¨äºä¸­æ–‡ç¼–ç 
data = pd.read_csv('./data/ä¹¦ç±è¯„ä»·.csv', encoding = 'gbk')
```

#### ğŸ“Œ æ•°æ®é¢„å¤„ç†

##### ğŸ” æ ‡ç­¾ç¼–ç <br>
`æœºå™¨å­¦ä¹ çš„æ¨¡å‹æ— æ³•ç›´æ¥ç†è§£æ–‡æœ¬æ ‡ç­¾ï¼Œæ‰€ä»¥éœ€è¦æŠŠæ ‡ç­¾ç¼–ç è½¬åŒ–ä¸ºæ•°å­—`ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬æ ‡ç­¾ï¼ˆ"å¥½è¯„"/"å·®è¯„"ï¼‰è½¬åŒ–ä¸ºæ•°å€¼ï¼ˆ1/0ï¼‰<br>
``` python
data['labels'] = np.where(data['è¯„ä»·'] == 'å¥½è¯„', 1, 0) # æ¡ä»¶æœç´¢dataä¸­çš„è¯„ä»·ï¼Œå‡å¦‚è¯„ä»·æ˜¯å¥½è¯„ï¼Œæ ‡å·1ï¼Œå¦åˆ™æ ‡å·0
y = data['labels'] # è¯„ä»·æ ‡å·ä½œä¸ºæ ‡ç­¾
```

##### ğŸ” åœç”¨è¯å¤„ç†ï¼š

æ¦‚å¿µï¼šè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„stop wordï¼Œç”¨äºæ–­å¥å’Œå»é™¤æ— æ„ä¹‰è¯æ±‡

ä½œç”¨ï¼šå¦‚"æˆ‘æ˜¯ä¸€ä¸ª..."ä¸­çš„"æ˜¯"ç­‰è¿æ¥è¯ï¼Œé€šè¿‡åœç”¨è¯è¡¨è¿‡æ»¤åå¯ä¿ç•™å…³é”®ä¸»è¯­ã€è°“è¯­ã€å®¾è¯­

å®ç°ï¼šåŠ è½½é¢„å®šä¹‰çš„åœç”¨è¯è¡¨ï¼ˆå¦‚stopwords.txtï¼‰å¹¶è¿›è¡Œå»é‡å¤„ç†<br>
``` python
stop_words = []
with open('./data/stopwords.txt', 'r', encoding = 'utf-8') as file:
  lines = file.readlines() # æŒ‰è¡Œè¯»å–
  stop_words = [line.strip() for line in lines] # å»é™¤æ¯è¡Œçš„ç©ºæ ¼
stop_words = list(set(stop_words)) # è½¬ä¸ºé›†åˆå»é‡ï¼Œå†è½¬å›åˆ—è¡¨
```

#### ğŸ“Œ ç‰¹å¾å·¥ç¨‹

##### ğŸ” åˆ†è¯å¤„ç†<br>
å°†å®Œæ•´å¥å­åˆ‡åˆ†ä¸ºç‹¬ç«‹è¯è¯­ï¼Œ`æå–å…³é”®ä¿¡æ¯ä»£è¡¨æ•´æ¡æ–‡æœ¬`ï¼Œå¦‚"æ‰‹æœºå¾ˆå¥½ç”¨"â†’["æ‰‹æœº","å¾ˆå¥½","ç”¨"]<br>
``` python
# å¯¼åŒ…
import jieba # åˆ†è¯å·¥å…·
# ä½¿ç”¨
comment_list = [','.join(jieba.lcut(line)) for line in data['å†…å®¹']] # å°†åˆ†è¯åçš„æ¯è¡Œç”¨é€—å·è¿æ¥
```

##### ğŸ” è¯é¢‘ç»Ÿè®¡<br>
ç»Ÿè®¡å„è¯è¯­å‡ºç°é¢‘ç‡ï¼Œ`æ„å»ºè¯é¢‘çŸ©é˜µä½œä¸ºå¥å­ç‰¹å¾`<br>
``` python
# å¯¼åŒ…
from sklearn.feature_extraction.text import CountVectorizer # ç”¨æ¥å°†æ–‡æœ¬è½¬åŒ–ä¸ºè¯é¢‘çŸ©é˜µ
# ä½¿ç”¨
vectorizer = CountVectorizer(stop_words = stop_words)
X = vectorizer.fit_transform(comment_list) # æŠŠcomment_listæ”¾å…¥scalerä¸­åˆ†è¯
names = vectorizer.get_feature_names_out()
X = X.toarray()
```

#### ğŸ“Œ æ¨¡å‹æ„å»º

##### ğŸ” ç®—æ³•é€‰æ‹©<br>
æœ‰ç›‘ç£å­¦ä¹ çš„åˆ†ç±»é—®é¢˜ï¼Œç”¨æœ´ç´ è´å¶æ–¯<br>
``` python
from sklearn.naive_bayes import MultinomialNB # å®ç°å¤šé¡¹æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨
```

##### ğŸ” å…³é”®å‚æ•°ï¼šalphaï¼ˆå¹³æ»‘ç³»æ•°ï¼‰ï¼Œç”¨äºå¤„ç†é›¶æ¦‚ç‡é—®é¢˜

### ä»£ç å®ç°

``` python
# 1.å¯¼åŒ…
import numpy as np  # ç”¨äºçŸ©é˜µè¿ç®—
import pandas as pd # ç”¨äºè¯»å–csvæ–‡æœ¬çš„æ•°æ®
import jieba # åˆ†è¯å·¥å…·
from sklearn.feature_extraction.text import CountVectorizer # ç‰¹å¾æå–ï¼Œå‘é‡åŒ–è®¡æ•°
from sklearn.model_selection import train_test_split # ç”¨äºåˆ†å‰²æ•°æ®é›†
from sklearn.naive_bayes import MultinomialNB # å¤šé¡¹æœ´ç´ è´å¶æ–¯
from sklearn.metrics import accuracy_score, classification_report # ç”¨äºæ¨¡å‹è¯„ä¼°

# 2.è·å–æ•°æ®ï¼Œæ•°æ®é¢„å¤„ç†ï¼Œç‰¹å¾å·¥ç¨‹
# 2.1 è¯»å–æ•°æ®
data = pd.read_csv('.\data\ä¹¦ç±è¯„ä»·.csv', encoding = 'utf-8')
# 2.2 æ·»åŠ æ ‡ç­¾åˆ—
data['labels'] = np.where(data['è¯„ä»·'] == 'å¥½è¯„', 1, 0) # æ¡ä»¶æœç´¢dataä¸­çš„è¯„ä»·ï¼Œå‡å¦‚è¯„ä»·æ˜¯å¥½è¯„ï¼Œæ ‡å·1ï¼Œå¦åˆ™æ ‡å·0
y = data['labels'] # è¯„ä»·æ ‡å·ä½œä¸ºæ ‡ç­¾
print(data)
# 2.3 è®¾ç½®åœç”¨è¯
stop_words = []
with open('./data/stopwords.txt', 'r', encoding = 'utf-8') as file:
  lines = file.readlines() # æŒ‰è¡Œè¯»å–
  stop_words = [line.strip() for line in lines] # å»é™¤æ¯è¡Œçš„ç©ºæ ¼
stop_words = list(set(stop_words)) # è½¬ä¸ºé›†åˆå»é‡ï¼Œå†è½¬å›åˆ—è¡¨
# 2.4 åˆ†è¯
comment_list = [','.join(jieba.lcut(line)) for line in data['å†…å®¹']] # å°†åˆ†è¯åçš„æ¯è¡Œç”¨é€—å·è¿æ¥
# 2.5 è¯é¢‘ç»Ÿè®¡
vectorizer = CountVectorizer(stop_words = stop_words)
X = vectorizer.fit_transform(comment_list) # æŠŠcomment_listæ”¾å…¥scalerä¸­åˆ†è¯
names = vectorizer.get_feature_names_out()
X = X.toarray()

# 3.æ•°æ®é›†åˆ’åˆ†
X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# 4.è®­ç»ƒæ¨¡å‹
# å®ä¾‹åŒ–KNNæ¨¡å‹
estimator = MultinomialNB(alpha=1)
# è®­ç»ƒæ¨¡å‹
estimator.fit(X_train, y_train)

# 5.æ¨¡å‹æµ‹è¯•
y_pred = estimator.predict(X_test)

# 6.æ¨¡å‹è¯„ä¼°
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred, zero_division=0))
```
